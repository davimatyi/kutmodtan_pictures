\section{Results}
\label{sec:results}
We have proven the validity and efficiency of SKBD, by evaluating it on two widely used datasets: Structured3D dataset\cite{zheng2020structured3d} and LSUN dataset\cite{zhang2015large}. We are using a standard metric for evaluation: Corner Error (CE) and Pixel Error (PE). Corner Error is computed by calculating the Euclidean distance between predicted corners and corresponding true corners. Pixel Error is calculated in a similar way, by overlaying the predicted and ground truth polygons, and measuring their difference as a percentage of total image pixels. For the sake of understandability in (fig refs), only wall edges are representing segments, and only wall planes are shown as polygons calculated from knots.

As all other highlighted implementations\cite{8451365}\cite{10350607} focus on Room Layout Reconstruction from a single image, the comparison tests do not fully leverage our solution. However, even in single viewpoint tests our method outperforms already existing state of the art algorithms. For comparison tests, we choose two Convolutional Network based approaches, referred to as MC-FCN\cite{8451365} and HGC\cite{10350607} to use as benchmarks. For MC-FCN, we trained the model based on the DeepLab-ResNet101\cite{chen2017deeplab} architecture.


\subsection{Results on Structured3D dataset}
In this test we are using the Strucured3D dataset\cite{zheng2020structured3d}, which is a photo-realistic, large-scale simulated set, containing 3D structure annotations. It consist of 3500 scenes, a total of 21835 rooms and 196515 images, and has a predefined training, validation and test data subset. The input images are resized to \(640 \times 480 \) resolution using bicubic interpolation. For calculating errors, we use the included information about the planes, such as polygon, 3D parameters and semantic label. We evaluate performance between the two proposed models and SKBD using simulated wireframe detection. Qualitative results are demonstrated in (fig ref). As seen here, FCNs can sometimes be confused by different surfaces and they might generate incorrect wireframes due to occlusions or clutter. Our method works around these uncertainties by utilizing clearly visible segments around said occlusions, and disregarding segments that have a low confidence level. We summarize the results of the test of set Structured3D in \autoref{table:structured3dperf}

\begin{table}[H]
\centering
\begin{tabular}{|c | c c |}
    \hline
    Type & $\epsilon[PE](\%)$ & $\epsilon[CE](\%)$ \\ [0.5 ex]
    \hline\hline
    MC-FCN & 6.91 & 4.98 \\
    HGC & 4.17 & 2.57 \\
    \hline
    SKBD & \textbf{1.89} & \textbf{1.34} \\
    \hline
\end{tabular}
\caption{Results on Structured3D dataset}
\label{table:structured3dperf}
\end{table}


\subsection{Results on LSUN dataset}
For this test we trained our SKBD descriptors using the relabeled LSUN dataset released by \cite{ren2017coarse}. The dataset consists of 4000 training images, 394 validation images and 1000 testing images. We manually grouped the images taken of the same room from multiple angles, and resized all images to \( 640 \times 480 \) resolution.

\begin{table}[H]
\centering
\begin{tabular}{|c | c c |}
    \hline
    Type & $\epsilon[PE](\%)$ & $\epsilon[CE](\%)$ \\ [0.5 ex]
    \hline\hline
    MC-FCN & 8.91 & 6.75 \\
    HGC & 5.29 & 3.44 \\
    \hline
    SKBD & \textbf{2.17} & \textbf{1.98} \\
    \hline
\end{tabular}
\caption{Results on LSUN dataset}
\label{table:lsunperf}
\end{table}



